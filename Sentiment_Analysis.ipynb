{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from nltk import pos_tag\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import bz2\n",
    "import os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'DE.ipynb', 'test.ft.txt.bz2', 'train.ft.txt.bz2']\n"
     ]
    }
   ],
   "source": [
    "files_pos = os.listdir('/Projects/Jupyter notebook/')\n",
    "print(files_pos)\n",
    "train_file = bz2.BZ2File('/Projects/Jupyter notebook/train.ft.txt.bz2')\n",
    "test_file=bz2.BZ2File('/Projects/Jupyter notebook/test.ft.txt.bz2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_lines=train_file.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_lines = [x.decode('utf-8') for x in train_file_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [x.split(' ', 1)[1][:-1] for x in train_file_lines]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = []\n",
    "documents = []\n",
    "\n",
    "import re\n",
    "i=0\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "#  j is adject, r is adverb, and v is verb\n",
    "#allowed_word_types = [\"J\",\"R\",\"V\"]\n",
    "allowed_word_types = [\"J\"]\n",
    "for i in range(0,len(train_labels)):\n",
    "    documents.append((train_sentences[i],train_labels[i]))\n",
    "len(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in documents[0:30000]:\n",
    "    # remove punctuations\n",
    "    cleaned = re.sub(r'[^(a-zA-Z)\\s]','', p[0])\n",
    "    \n",
    "    # tokenize \n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    \n",
    "    # remove stopwords \n",
    "    stopped = [w for w in tokenized if not w in stop_words]\n",
    "    \n",
    "    # parts of speech tagging for each word \n",
    "    pos = nltk.pos_tag(stopped)\n",
    "    \n",
    "    # make a list of  all adjectives identified by the allowed word types list above\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words.append(w[0].lower())\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_documents = open(\"pickled_algos/documents.sav\",\"wb\")\n",
    "pickle.dump(documents, save_documents)\n",
    "save_documents.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204954"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'good': 8025, 'great': 6906, 'best': 2917, 'many': 2818, 'much': 2738, 'bad': 2464, 'little': 2464, 'first': 2359, 'new': 2264, 'old': 1963, ...})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a frequency distribution of each adjectives. \n",
    "BOW = nltk.FreqDist(all_words)\n",
    "BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('beautiful', 'litany')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# listing the 5000 most frequent words\n",
    "word_features = list(BOW.keys())[:5000]\n",
    "word_features[0], word_features[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_word_features = open(\"pickled_algos/word_features5k.sav\",\"wb\")\n",
    "pickle.dump(word_features, save_word_features)\n",
    "save_word_features.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "# function to create a dictionary of features for each review in the list document.\n",
    "# The keys are the words in word_features \n",
    "# The values of each key are either true or false for wether that feature appears in the review or not\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Creating features for each review\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents[0:30000]]\n",
    "#featuresets_test=[(find_features(rev), category) for (rev, category) in document_test[0:20000]]\n",
    "# Shuffling the documents \n",
    "random.shuffle(featuresets)\n",
    "print(len(featuresets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set : 25000 \n",
      "testing_set : 5000\n"
     ]
    }
   ],
   "source": [
    "training_set = featuresets[0:25000]\n",
    "testing_set = featuresets[25000:]\n",
    "print( 'training_set :', len(training_set), '\\ntesting_set :', len(testing_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 81.14\n",
      "Most Informative Features\n",
      "                  refund = True                0 : 1      =    102.9 : 1.0\n",
      "                   waste = True                0 : 1      =     31.2 : 1.0\n",
      "                pathetic = True                0 : 1      =     28.2 : 1.0\n",
      "                 rubbish = True                0 : 1      =     23.7 : 1.0\n",
      "             embarrassed = True                0 : 1      =     21.6 : 1.0\n",
      "               defective = True                0 : 1      =     21.4 : 1.0\n",
      "             pretentious = True                0 : 1      =     20.9 : 1.0\n",
      "                  poorly = True                0 : 1      =     19.9 : 1.0\n",
      "                rambling = True                0 : 1      =     19.4 : 1.0\n",
      "                   worst = True                0 : 1      =     18.2 : 1.0\n",
      "                travesty = True                0 : 1      =     18.0 : 1.0\n",
      "               worthless = True                0 : 1      =     17.8 : 1.0\n",
      "                   awful = True                0 : 1      =     16.8 : 1.0\n",
      "                 publish = True                0 : 1      =     16.6 : 1.0\n",
      "                  shoddy = True                0 : 1      =     16.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['refund', 'waste', 'pathetic', 'rubbish', 'embarrassed', 'defective', 'pretentious', 'poorly', 'rambling', 'worst', 'travesty', 'worthless', 'awful', 'publish', 'shoddy', 'letdown', 'timeless', 'tripe', 'unfunny', 'unusable', 'pointless', 'atrocious', 'disappointment', 'useless', 'forgettable', 'cancel', 'clumsy', 'horrendous', 'idiotic', 'warped', 'lousy', 'wasted', 'offensive', 'formatted', 'dreadful', 'disappointing', 'laughable', 'unreadable', 'trash', 'irrelevant', 'superficial', 'applicable', 'disservice', 'formulaic', 'jammed', 'nonsensical', 'horrible', 'shines', 'uninteresting', 'garbage', 'poor', 'worse', 'skeptical', 'drunk', 'underrated', 'coolest', 'rural', 'warranty', 'advertisement', 'ridiculous', 'delightful', 'cheated', 'overrated', 'lame', 'misleading', 'incomprehensible', 'terrible', 'abysmal', 'fluke', 'suction', 'trigger', 'hassle', 'incorrect', 'adventurous', 'gem', 'contrived', 'inferior', 'launch', 'talentless', 'watered', 'redeem', 'stink', 'downside', 'energetic', 'para', 'destiny', 'harmony', 'shallow', 'delicious', 'banal', 'deceptive', 'irritated', 'vague', 'superb', 'finest', 'trite', 'artsy', 'fest', 'gibberish', 'passable']\n"
     ]
    }
   ],
   "source": [
    "# Printing the most important features \n",
    "\n",
    "mif = classifier.most_informative_features()\n",
    "\n",
    "mif = [a for a,b in mif]\n",
    "print(mif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting predictions for the testing set by looping over each reviews featureset tuple\n",
    "# The first elemnt of the tuple is the feature set and the second element is the label \n",
    "ground_truth = [r[1] for r in testing_set]\n",
    "\n",
    "preds = [classifier.classify(r[0]) for r in testing_set]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8114"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(ground_truth, preds, labels = [0, 1], average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers for an ensemble model: \n",
    "Naive Bayes (NB)\n",
    "Multinomial NB\n",
    "Bernoulli NB\n",
    "Logistic Regression\n",
    "Stochastic Gradient Descent Classifier - SGD\n",
    "Support Vector Classification - SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB_classifier accuracy percent: 81.08\n"
     ]
    }
   ],
   "source": [
    "#print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "#classifier.show_most_informative_features(15)\n",
    "\n",
    "MNB_clf = SklearnClassifier(MultinomialNB())\n",
    "MNB_clf.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_clf, testing_set))*100)\n",
    "\n",
    "BNB_clf = SklearnClassifier(BernoulliNB())\n",
    "BNB_clf.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BNB_clf, testing_set))*100)\n",
    "\n",
    "LogReg_clf = SklearnClassifier(LogisticRegression())\n",
    "LogReg_clf.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogReg_clf, testing_set))*100)\n",
    "\n",
    "SGD_clf = SklearnClassifier(SGDClassifier())\n",
    "SGD_clf.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGD_clf, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_clf = SklearnClassifier(SVC()\n",
    "SVC_clf.train(training_set)\n",
    "print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_clf, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing all models using pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pickle(c, file_name): \n",
    "    save_classifier = open(file_name, 'wb')\n",
    "    pickle.dump(c, save_classifier)\n",
    "    save_classifier.close()\n",
    "\n",
    "classifiers_dict = {'ONB': [classifier, 'pickled_algos/ONB_clf.sav'],\n",
    "                    'MNB': [MNB_clf, 'pickled_algos/MNB_clf.sav'],\n",
    "                    'BNB': [BNB_clf, 'pickled_algos/BNB_clf.sav'],\n",
    "                    'LogReg': [LogReg_clf, 'pickled_algos/LogReg_clf.sav'],\n",
    "                    'SGD': [SGD_clf, 'pickled_algos/SGD_clf.sav']} \n",
    "                    #'SVC': [SVC_clf, 'pickled_algos/SVC_clf.pickle']}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for clf, listy in classifiers_dict.items(): \n",
    "    create_pickle(listy[0], listy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''acc_scores = {}\n",
    "for clf, listy in classifiers_dict.items(): \n",
    "    # getting predictions for the testing set by looping over each reviews featureset tuple\n",
    "    # The first elemnt of the tuple is the feature set and the second element is the label \n",
    "    acc_scores[clf] = accuracy_score(ground_truth, predictions[clf])\n",
    "    print(f'Accuracy_score {clf}: {acc_scores[clf]}')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "ground_truth = [r[1] for r in testing_set]\n",
    "predictions = {}\n",
    "f1_scores = {}\n",
    "for clf, listy in classifiers_dict.items(): \n",
    "    # getting predictions for the testing set by looping over each reviews featureset tuple\n",
    "    # The first elemnt of the tuple is the feature set and the second element is the label \n",
    "    predictions[clf] = [listy[0].classify(r[0]) for r in testing_set]\n",
    "    f1_scores[clf] = f1_score(ground_truth, predictions[clf], labels = [0, 1], average = 'micro')\n",
    "    print(f'f1_score {clf}: {f1_scores[clf]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "\n",
    "# Defininig the ensemble model class \n",
    "\n",
    "class EnsembleClassifier(ClassifierI):\n",
    "    \n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "    \n",
    "    # returns the classification based on majority of votes\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "    # a simple measurement the degree of confidence in the classification \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading all models using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all classifiers from the pickled files \n",
    "\n",
    "# function to load models given filepath\n",
    "def load_model(file_path): \n",
    "    classifier_f = open(file_path, \"rb\")\n",
    "    classifier = pickle.load(classifier_f)\n",
    "    classifier_f.close()\n",
    "    return classifier\n",
    "\n",
    "\n",
    "# Original Naive Bayes Classifier\n",
    "ONB_Clf = load_model('pickled_algos/ONB_clf.sav')\n",
    "\n",
    "# Multinomial Naive Bayes Classifier \n",
    "MNB_Clf = load_model('pickled_algos/MNB_clf.sav')\n",
    "\n",
    "\n",
    "# Bernoulli  Naive Bayes Classifier \n",
    "BNB_Clf = load_model('pickled_algos/BNB_clf.sav')\n",
    "\n",
    "# Logistic Regression Classifier \n",
    "LogReg_Clf = load_model('pickled_algos/LogReg_clf.sav')\n",
    "\n",
    "# Stochastic Gradient Descent Classifier\n",
    "SGD_Clf = load_model('pickled_algos/SGD_clf.sav')\n",
    "\n",
    "\n",
    "\n",
    "# Initializing the ensemble classifier \n",
    "ensemble_clf = EnsembleClassifier(ONB_Clf, MNB_Clf, BNB_Clf, LogReg_Clf, SGD_Clf)\n",
    "\n",
    "# List of only feature dictionary from the featureset list of tuples \n",
    "feature_list = [f[0] for f in testing_set]\n",
    "\n",
    "# Looping over each to classify each review\n",
    "ensemble_preds = [ensemble_clf.classify(features) for features in feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(ground_truth, ensemble_preds, average = 'micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Sentiment Analysis\n",
    "\n",
    "Using the sentiment function we can classify individual reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to do classification a given review and return the label a\n",
    "# and the amount of confidence in the classifications\n",
    "def sentiment(text):\n",
    "    feats = find_features(text)\n",
    "    return ensemble_clf.classify(feats), ensemble_clf.confidence(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sentiment analysis of reviews of captain marvel found on rotten tomatoes\n",
    "text_a = '''Wow super I so very happy thank you filpcord'''\n",
    "text_b = '''Got product on time, but didn't use it for few days, when I started it saw that water was leaking from somewhere, fan holder was broken, swing vent was broken, a complete defective piece\n",
    "When I complained to Flipkart they said we cannot help please contact Orient customer care!!'''\n",
    "text_c = '''It was lacking, a bit flat, and I'm honestly concerned about how she will enter\n",
    "            the Marvel Cinematic Universe...it's so concerned with being a feminist film that \n",
    "            it forgets how to be a superhero movie.'''\n",
    "text_d = '''The film may be about women breaking their shackles, but the lead actress feels kept \n",
    "            in check for much of the picture. Humor winds up being provided by Samuel Jackson's Nick \n",
    "            Fury, heart by Lashana Lynch's Maria Rambeau, and pathos by...well, it ain't Larson'''\n",
    "text_e = '''\"Everything was beautiful and nothing hurt\"'''\n",
    "\n",
    "sentiment(text_a), sentiment(text_b), sentiment(text_c), sentiment(text_d), sentiment(text_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
